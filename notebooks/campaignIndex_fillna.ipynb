{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from preprocessing.data_preparation import read_data\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "\n",
    "# merge items and train data to a dataFrame\n",
    "def merge_data(input_path, items_path, output_path):\n",
    "    tdf = read_data(input_path)\n",
    "    idf = read_data(items_path)\n",
    "    print('data read successfully!')\n",
    "    output = Path(output_path)\n",
    "    if not output.is_file():\n",
    "        train_merged = tdf.copy()\n",
    "        train_merged = train_merged.merge(tdf.merge(idf, how='left', on='pid', sort=False))\n",
    "        pd.to_pickle(train_merged, output_path)\n",
    "        return train_merged\n",
    "    else:\n",
    "        return pd.read_pickle(output_path)\n",
    "\n",
    "\n",
    "def extract_numbers_from_content(input):\n",
    "    x_index = input.find('X')\n",
    "    if input == 'L   125':\n",
    "        return 1, 1, 125\n",
    "    if x_index == -1:\n",
    "        if input == 'PAK':\n",
    "            return 1, 1, 1\n",
    "        return 1, 1, input\n",
    "    second_part = input[x_index + 1: len(input)]\n",
    "    x_second_index = second_part.find('X')\n",
    "    if x_second_index == -1:\n",
    "        return 1, input[0: x_index], second_part\n",
    "    return input[0: x_index], second_part[0: x_second_index], second_part[x_second_index + 1: len(second_part)]\n",
    "\n",
    "\n",
    "unit_map = {\n",
    "    'KG': 1000,\n",
    "    'ST': 6350,\n",
    "    'P': 454,\n",
    "    'M': 100,\n",
    "    'L': 1000,\n",
    "    'G': 1,\n",
    "    'CM': 1,\n",
    "    'ML': 1,\n",
    "}\n",
    "\n",
    "\n",
    "def unit_converter(row):\n",
    "    return row['content_3'] * unit_map[row['unit']]\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    output = Path('data/unit_fixed.pkl')\n",
    "    if not output.is_file():\n",
    "        # example of using merge_data function for train dataset\n",
    "        mrg = merge_data('data/train.csv', 'data/items.csv', 'data/train_merged.pkl')\n",
    "\n",
    "        # add count feature (revenue/price)\n",
    "        mrg['count'] = mrg.revenue / mrg.price\n",
    "\n",
    "        # uppercase all pharmForm values\n",
    "        mrg['pharmForm'] = mrg['pharmForm'].str.upper()\n",
    "        # extract pharmForm values as binary feature and adding them to dataset\n",
    "        mrg = pd.concat([mrg, pd.get_dummies(mrg['pharmForm'])], axis=1)\n",
    "        mrg = mrg.drop('pharmForm', 1)\n",
    "\n",
    "        # split count of packs and amount of each to separate columns\n",
    "        extracted_numbers = mrg['content'].apply(extract_numbers_from_content)\n",
    "        extracted_numbers = pd.DataFrame(extracted_numbers.tolist(), columns=['content_1', 'content_2', 'content_3'],\n",
    "                                         index=extracted_numbers.index)\n",
    "        extracted_numbers['content_1'] = pd.to_numeric(extracted_numbers['content_1'])\n",
    "        extracted_numbers['content_2'] = pd.to_numeric(extracted_numbers['content_2'])\n",
    "        extracted_numbers['content_3'] = pd.to_numeric(extracted_numbers['content_3'])\n",
    "        mrg = pd.concat([mrg, extracted_numbers], axis=1)\n",
    "        mrg = mrg.drop('content', 1)\n",
    "\n",
    "        mrg['content_3'] = mrg.apply(unit_converter, axis=1)\n",
    "        mapping = {'KG': 'G', 'ST': 'G', 'P': 'G', 'L': 'ML', 'M': 'CM'}\n",
    "        mrg = mrg.replace({'unit': mapping})\n",
    "        pd.to_pickle(mrg, '../data/unit_fixed.pkl')\n",
    "        print('units converted')\n",
    "    else:\n",
    "        mrg = pd.read_pickle('data/unit_fixed.pkl')\n",
    "\n",
    "    # fill campaignIndex with D and then get dummy binary values of each category index\n",
    "    # mrg['campaignIndex'].fillna('D', inplace=True)\n",
    "    # mrg = pd.concat([mrg, pd.get_dummies(mrg['campaignIndex'])], axis=1)\n",
    "\n",
    "    # mrg = pd.concat([mrg, pd.get_dummies(mrg['group'])], axis=1)\n",
    "    return mrg\n",
    "\n",
    "\n",
    "def predict_competitor(all_data):\n",
    "    train = all_data[pd.notnull(all_data['competitorPrice'])]\n",
    "    kf = KFold(n_splits=10)\n",
    "    estimator = XGBRegressor()\n",
    "    x = train.drop('competitorPrice', 1)\n",
    "    y = train['competitorPri1ce']\n",
    "    scores = cross_val_score(estimator,\n",
    "                             x,\n",
    "                             y,\n",
    "                             cv=kf,\n",
    "                             scoring=make_scorer(mean_squared_error))\n",
    "    print(scores)\n",
    "\n",
    "\n",
    "data = prepare_dataset()\n",
    "# from scipy.stats import pearsonr\n",
    "#\n",
    "# print(data['category'].fillna(0))\n",
    "# print(pearsonr(data['category'].fillna(0), data['count']))\n",
    "\n",
    "\n",
    "# TODO handle features: category, group, competitor\n",
    "# TODO Random Forrest on server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_missing = data[pd.isnull(data['campaignIndex'])]['lineID']\n",
    "adFlag_missing = data[data['adFlag'] == 0]['lineID']\n",
    "# print(campaign_missing)\n",
    "# print(data['adFlag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2287968"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n1880176\n"
     ]
    }
   ],
   "source": [
    "print(len(campaign_missing))\n",
    "print(len(adFlag_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n"
     ]
    }
   ],
   "source": [
    "intersections = pd.Series(\n",
    "    list(set(campaign_missing).intersection(\n",
    "        set(adFlag_missing))))\n",
    "print(len(intersections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsp/anaconda3/lib/python3.5/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n  warn('The line search algorithm did not converge', LineSearchWarning)\n/home/dsp/anaconda3/lib/python3.5/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n          n_jobs=1, penalty='l2', random_state=None, solver='newton-cg',\n          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To be filled with similar product campaign Index\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(solver='newton-cg',\n",
    "                        multi_class='multinomial')\n",
    "train_data = data[pd.notnull(data['campaignIndex'])]\n",
    "lr.fit(train_data[['pid', 'manufacturer']],\n",
    "       train_data['campaignIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the 600K part\n",
    "pred = lr.predict(data[pd.isnull(\n",
    "    data['campaignIndex'])][['pid', 'manufacturer']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B']\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(data[pd.notnull(data['campaignIndex'])][['pid', 'manufacturer']])\n",
    "print(pd.Series(pred).unique())\n",
    "# Not good enough, trying naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1651555\n0          NaN\n1            C\n2          NaN\n3          NaN\n4          NaN\n5          NaN\n6          NaN\n7          NaN\n8          NaN\n9          NaN\n10           C\n11         NaN\n12         NaN\n13         NaN\n14         NaN\n15           A\n16           B\n17         NaN\n18         NaN\n19         NaN\n20         NaN\n21         NaN\n22         NaN\n23         NaN\n24           B\n25         NaN\n26         NaN\n27         NaN\n28         NaN\n29         NaN\n          ... \n2755973    NaN\n2755974    NaN\n2755975    NaN\n2755976    NaN\n2755977    NaN\n2755978    NaN\n2755979    NaN\n2755980    NaN\n2755981    NaN\n2755982    NaN\n2755983    NaN\n2755984    NaN\n2755985    NaN\n2755986    NaN\n2755987      B\n2755988    NaN\n2755989    NaN\n2755990    NaN\n2755991      B\n2755992    NaN\n2755993    NaN\n2755994    NaN\n2755995    NaN\n2755996    NaN\n2755997    NaN\n2755998    NaN\n2755999    NaN\n2756000    NaN\n2756001      A\n2756002      A\nName: campaignIndex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print(intersections)\n",
    "# These are lineIDs with missing campaignIndex and adFlag=0\n",
    "print(len(intersections))\n",
    "ind = data.lineID.isin(intersections.tolist())\n",
    "print(len(data[ind]['campaignIndex']))\n",
    "print(data['campaignIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            D\n1            C\n2            D\n3          NaN\n4            D\n5            D\n6          NaN\n7            D\n8            D\n9          NaN\n10           C\n11         NaN\n12           D\n13           D\n14           D\n15           A\n16           B\n17           D\n18           D\n19           D\n20           D\n21           D\n22           D\n23           D\n24           B\n25           D\n26           D\n27           D\n28         NaN\n29           D\n          ... \n2755973      D\n2755974      D\n2755975      D\n2755976      D\n2755977      D\n2755978    NaN\n2755979      D\n2755980      D\n2755981      D\n2755982      D\n2755983      D\n2755984      D\n2755985    NaN\n2755986      D\n2755987      B\n2755988      D\n2755989      D\n2755990      D\n2755991      B\n2755992      D\n2755993      D\n2755994      D\n2755995      D\n2755996      D\n2755997    NaN\n2755998      D\n2755999      D\n2756000      D\n2756001      A\n2756002      A\nName: campaignIndex, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# To be filled with D\n",
    "data['campaignIndex']. \\\n",
    "    fillna(data[ind]['campaignIndex'].fillna('D'), inplace=True)\n",
    "print(data['campaignIndex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the rest with naive bayes prediction\n",
    "train_data = data[pd.notnull(data['campaignIndex'])]\n",
    "test_data = data[pd.isnull(data['campaignIndex'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2119590\n636413\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "naive_bayes_clf = GaussianNB()\n",
    "naive_bayes_clf.fit(train_data[['pid', 'manufacturer', 'rrp']],\n",
    "                    train_data['campaignIndex'])\n",
    "predictions = naive_bayes_clf.predict(\n",
    "    test_data[['pid', 'manufacturer', 'rrp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "data.ix[data['lineID'].isin(test_data['lineID']),\n",
    "        'campaignIndex'] = predictions\n",
    "print(len(data[pd.isnull(data['campaignIndex'])]))\n",
    "# campaignIndex filled completely"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}